{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = SparkContext(appName = 'SparkMLCrossVal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "! echo $PYSPARK_SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.Builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download a DataSet ( Breast Cancer diagnosis )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-03-18 14:29:59--  https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249, 80c3:af9:e::c29e:d69b\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 124103 (121K) [text/plain]\n",
      "Saving to: ‘wdbc.data’\n",
      "\n",
      "wdbc.data           100%[===================>] 121.19K  78.8KB/s    in 1.5s    \n",
      "\n",
      "2019-03-18 14:30:02 (78.8 KB/s) - ‘wdbc.data’ saved [124103/124103]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a row\n",
    "\n",
    "data = []\n",
    "\n",
    "with open('wdbc.data') as infile:\n",
    "    for line in infile:\n",
    "        tokens = line.rstrip('\\n').split(',')\n",
    "        y = tokens[1]\n",
    "        features = Vectors.dense([float(x) for x in tokens[2:]])\n",
    "        \n",
    "        data.append((y, features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark.createDataFrame(data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCol = \"label\", outputCol = \"labelIndexed\")\n",
    "si_model = stringIndexer.fit(inputDF)\n",
    "\n",
    "inputDF2 = si_model.transform(inputDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------+\n",
      "|label|            features|labelIndexed|\n",
      "+-----+--------------------+------------+\n",
      "|    M|[17.99,10.38,122....|         1.0|\n",
      "|    M|[20.57,17.77,132....|         1.0|\n",
      "|    M|[19.69,21.25,130....|         1.0|\n",
      "|    M|[11.42,20.38,77.5...|         1.0|\n",
      "|    M|[20.29,14.34,135....|         1.0|\n",
      "|    M|[12.45,15.7,82.57...|         1.0|\n",
      "|    M|[18.25,19.98,119....|         1.0|\n",
      "|    M|[13.71,20.83,90.2...|         1.0|\n",
      "|    M|[13.0,21.82,87.5,...|         1.0|\n",
      "|    M|[12.46,24.04,83.9...|         1.0|\n",
      "|    M|[16.02,23.24,102....|         1.0|\n",
      "|    M|[15.78,17.89,103....|         1.0|\n",
      "|    M|[19.17,24.8,132.4...|         1.0|\n",
      "|    M|[15.85,23.95,103....|         1.0|\n",
      "|    M|[13.73,22.61,93.6...|         1.0|\n",
      "|    M|[14.54,27.54,96.7...|         1.0|\n",
      "|    M|[14.68,20.13,94.7...|         1.0|\n",
      "|    M|[16.13,20.68,108....|         1.0|\n",
      "|    M|[19.81,22.15,130....|         1.0|\n",
      "|    B|[13.54,14.36,87.4...|         0.0|\n",
      "+-----+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputDF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree = DecisionTreeClassifier(labelCol = \"labelIndexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = [decisionTree])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(decisionTree.maxDepth, [1,2,4,5,6,7,8])\\\n",
    "    .build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"labelIndexed\", predictionCol = \"prediction\", metricName = \"accuracy\")\n",
    "\n",
    "crossVal = CrossValidator(estimator = pipeline,\n",
    "                         estimatorParamMaps = paramGrid,\n",
    "                         evaluator = evaluator,\n",
    "                         numFolds = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaParams.__del__ at 0x7f2a370b2268>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/pyspark/ml/wrapper.py\", line 105, in __del__\n",
      "    SparkContext._active_spark_context._gateway.detach(self._java_obj)\n",
      "AttributeError: 'MulticlassClassificationEvaluator' object has no attribute '_java_obj'\n"
     ]
    }
   ],
   "source": [
    "cvModel = crossVal.fit(inputDF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8882065195104409,\n",
       " 0.9138000039511773,\n",
       " 0.9252680190482211,\n",
       " 0.9182471904144894,\n",
       " 0.9154205967183304,\n",
       " 0.9135125527343403,\n",
       " 0.9135125527343403]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_444da0de9c538697890e) of depth 4 with 25 nodes\n"
     ]
    }
   ],
   "source": [
    "print (cvModel.bestModel.stages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
